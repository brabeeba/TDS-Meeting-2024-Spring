<!DOCTYPE html>
<html lang="en">
	<head>
    	<meta charset="utf-8">
    	<title>TDS Meeting, Spring, 2024</title>
    	<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="">
    	<meta name="author" content="">

    	<!-- Le styles -->
    	<link href="./bootstrapFlatly.min.css" rel="stylesheet">
    	<style>
      	body {
        	/*padding-top: 60px;  60px to make the container go all the way to the bottom of the topbar */
      	}
    	</style>
    	<link href="./bootstrap-responsive.min.css" rel="stylesheet">
    	<link href="./main.css" rel="stylesheet">
	</head>
	<body>
	<div class="container">
	<div class="row-fluid">
	  <h3>TDS Meeting, Spring, 2024</h3>
   </div>
	<div class="row-fluid voffset1">
      	<span class="icon-time"></span><strong>Time:</strong> Friday, 1:00–2:30PM 
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-map-marker"></span><strong>Location:</strong> Room G631<br>
	  
	</div>
	 <div class="row-fluid voffset1">
	  <span class="icon-envelope"></span><strong>Organizers:</strong>Nancy Lynch<br>
	</div>


<div class="row-fluid voffset1">
	<div class="col-md-10 colp-lg-8" >
   		<img src="./triple.png" class="img-rounded img-responsive" alt="my photo">
</div>
	<div class="col-md-2 colp-lg-8" >
   		<img src="./nano.PNG" class="img-rounded img-responsive" alt="my photo">
</div>
  <div class="row voffset1">
    <div class="col-md-12">
      <section id="publications">
      	
      	<!-- <h3>Background reading/watching</h3>
      	<ul>
      		<li>
      			<a href="https://b-ok.cc/book/2477222/4a05ed">Principles of Neural Science</a>
      		</li>
      		<li>
      			<a href="https://nancysbraintalks.mit.edu/">Nancy Kanwisher online course</a>
      		</li>
      		<li>
      			<a href="https://computationandbrain.github.io/about/">Christos Papadimitriou on-line course ("Computation and the Brain")</a>
      		</li>
      		<li>
      			<a href="http://cseweb.ucsd.edu/~dasgupta/254-neural-ul/index.html">Sanjoy Dasgupta’s course (“Neurally-inspired unsupervised learning")</a>
      		</li>
      		<li>
      			<a href="https://www.frontiersin.org/research-topics/39/spike-timing-dependent-plasticity">Reviews on Spike-timing dependent plasticity</a>
      		</li>
      	</ul> -->


        <h3>(Tentative) Schedule</h3>


		<p>(3/8) Theory and model of thalamocortical processing in decision-making under uncertainty: Brabeeba Wang</p>
		<p>Abstract: 
Animals flexibly select actions that maximize future rewards despite facing uncertainty in sensory inputs, action-outcome associations or contexts. The computational and circuit mechanisms underlying this ability are poorly understood. A clue to such computations can be found in the neural systems involved in representing sensory features, sensorimotor-outcome associations and contexts. Specifically, the basal ganglia (BG) have been implicated in forming sensorimotor-outcome associations while the thalamocortical loop between the prefrontal cortex (PFC) and mediodorsal thalamus (MD) has been shown to engage in contextual representations. Interestingly, both human and non-human animal experiments indicate that the MD represents different forms of uncertainty. However, finding evidence for uncertainty representation gives little insight into how it is utilized to drive behavior. Normative theories have excelled at providing such computational insights. However, despite their computational insight and ability to fit behaviors, normative models cannot be directly related to neural mechanisms. Therefore, a critical gap exists between what we know about the neural representation of uncertainty on one end and the computational functions uncertainty serves in cognition. This gap can be filled with mechanistic neural models that can approximate normative models as well as generate experimentally observed neural representations.</p>
<p>In this work, we build a mechanistic cortico-thalamo-BG loop network model that directly fills this gap. The model includes computationally-relevant mechanistic details of both BG and thalamocortical circuits such as distributional activities of dopamine and thalamocortical projection modulating cortical effective connectivity and plasticity via interneurons. We show that our network can more efficiently and flexibly explore various environments compared to commonly used machine learning algorithms and we show that the mechanistic features we include are crucial for handling different types of uncertainty in decision-making. Furthermore, through derivation and mathematical proofs, we approximate our models to two novel normative theories. We show mathematically the first has near-optimal performance on bandit tasks. The second is a generalization on the well-known CUMSUM algorithm, which is known to be optimal on single change point detection tasks. Our normative model expands on this by detecting multiple sequential contextual changes. To our knowledge, our work is the first to link computational insights, normative models and neural realization together in decision-making under various forms of uncertainty.</p>
		
		<p>(3/15) Modeling Feasible Locomotion of Nanobots for Cancer Detection and Treatment:  Noble Harasha
			
		</p>
		<p>
			Abstract: Deploying nanoscopic particles and robots in the human body promises increasingly selective and less toxic drug delivery. We consider the problem of a homogeneous swarm of nanobots locating a singular cancerous region and treating it by releasing some onboard payload of drugs. At nanoscale, the computation, communication, sensing, and locomotion capabilities of individual agents are extremely limited, noisy, and/or nonexistent. Inspired by actual nanoscopic vesicles which, when in the presence of an external chemical gradient, tend towards areas of higher concentration by means of self-propulsion, we present a feasible model of locomotion for our agents. We model this process of noisy gradient ascent as two precise algorithms, and present accompanying simulation and analytical results. Finally, with an established model of locomotion, we extend our environment setup to consider the problem of locating multiple cancer sites and allocating treatment efficiently and evenly across the sites.
		</p>

		<p>(4/5) Flipping a Coin in the Brain, & Other Cognitive Primitives with Assemblies of Neurons:  Robert (Max) Dabagia
			
		</p>
		<p>
			Abstract: Even as machine learning exceeds human-level performance in 
many applications, the generality, robustness, and rapidity of the 
brain's learning capabilities remain unmatched. How the mass of cells 
we call the brain gives rise to cognition is a central open question 
in neuroscience, inextricable from the study of intelligence itself. A 
simple formal model of neural activity (NEMO) was proposed by 
Papadimitriou and Vempala [2020] and has been subsequently shown, 
through both mathematical proofs and simulations, to be capable of 
implementing certain simple cognitive operations via the creation and 
manipulation of assemblies of neurons -- groups of neurons which are 
hypothesized to coordinate to encode memories, ideas, and other 
cognitive primitives. However, many intelligent behaviors rely on the 
ability to recognize, store, manipulate, and generate temporal 
sequences of stimuli and action (planning, language, navigation, to 
list a few).</p><p>

In this talk, I will discuss how sequentiality is readily captured in 
NEMO, and the resulting range of computations on sequences of 
assemblies that can be realized. Our most basic result is that 
repeated presentation of a sequence of stimuli will lead to 
memorization of the corresponding sequence of neural assemblies: Upon 
future presentation of any stimulus in the sequence, the corresponding 
assembly and its subsequent ones will be recalled, one after the 
other, until the end of the sequence. Building upon this idea, NEMO 
can simulate an arbitrary finite state machine (FSM), and an FSM can 
even be learned from repeated presentation of its transitions. As a 
consequence, NEMO is Turing complete. Finally, I will show how ambient 
randomness can be "amplified" to randomly choose one of a set of 
assemblies to fire, paving the way to probabilistic automata, sampling 
from a distribution, and other key cognitive applications. Crucially, 
all of these results provably emerge within a biologically faithful 
model of the brain, where all activity is driven and controlled by 
neurons.
		</p>
		<p>
			<ul>
				<li>
					<a href="https://arxiv.org/abs/2306.03812">Computation with Sequences in a Model of the Brain. Max Dabagia, Christos H. Papadimitriou, Santosh S. Vempala. 2023.</a>
				</li>
				<li>
					<a href="https://proceedings.mlr.press/v178/dabagia22a.html">Assemblies of neurons learn to classify well-separated distributions. Max Dabagia, Santosh S Vempala, Christos Papadimitriou.  Proceedings of Thirty Fifth Conference on Learning Theory. 2022.</a>
				</li>
				<li>
					<a href="https://www.pnas.org/doi/10.1073/pnas.2001893117">Brain computation by assemblies of neurons. Christos H. Papadimitriou, Santosh S. Vempala, Daniel Mitropolsky,0 Wolfgang Maass. PNAS. 2020.</a>
				</li>
			</ul>
		</p>


		
		<p>(4/12) Optimal routing to cerebellum-like structures: Brabeeba Wang
			<ul>
				
				<li>
					<a href="https://www.nature.com/articles/s41593-023-01403-7">Optimal routing to cerebellum-like structures. Samuel P. Muscinelli, Mark J. Wagner & Ashok Litwin-Kumar. Nature Neuroscience. 2023.</a>
				</li>
				
			</ul>
		</p>
		
		<p>(4/19) Neural Manifold Capacity Captures Representation Geometry, Correlations, and Task-Efficiency Across Species and Behaviors: Chi-Ning Chou
			<ul>
				
				
				<li>
					<a href="https://www.biorxiv.org/content/10.1101/2024.02.26.582157v1">Neural Manifold Capacity Captures Representation Geometry, Correlations, and Task-Efficiency Across Species and Behaviors. Chi-Ning Chou, Luke Arend, Albert J. Wakhloo, Royoung Kim, Will Slatton and SueYeon Chung. 2024.</a>
				</li>
				
			</ul>
		</p>
		<p>(4/26) Neuronal calcium spikes enable vector inversion in the Drosophila brain: Brabeeba Wang
			<ul>
				<li>
					<a href="https://www.biorxiv.org/content/10.1101/2023.11.24.568537v3.abstract">Neuronal calcium spikes enable vector inversion in the Drosophila brain. Itzel G. Ishida,  Sachin Sethi,  Thomas L. Mohren,  L.F. Abbott and Gaby Maimon. 2023.</a>
				</li>
			
				
			</ul>
		</p>

		<p>(5/3) The benefits of noise in neural systems: Sabrina Drammis, Nancy Lynch
			<ul>
				<li>
					<a href="https://www.pnas.org/doi/full/10.1073/pnas.1320116110">A theory for how sensorimotor skills are learned and retained in noisy and nonstationary neural circuits. Robert Ajemian, Alessandro D’Ausilio, Helene Moorman, and Emilio Bizzi. PNAS. 2013.</a>
				</li>
				<li>
					<a href="342c53d4cb8f5be57a90471f7edce5fda30b-1.pdf">The benefits of noise in neural systems: bridging theory and experiment. Mark D. McDonnell and Lawrence M. Ward. Nature Review Neuroscience. 2011.</a>
				</li>
			
				
			</ul>
		</p>



			<a href="https://accessibility.mit.edu/">Accessibility</a>
		
	<!--	<h4><strong>Unscheduled but need/want to fit in somewhere</strong></h4>
		<p>
			<ul>
				<li><a href="https://igi-web.tugraz.at/people/maass/psfiles/154.pdf">What Can a Neuron Learn with Spike-Timing-Dependent Plasticity?</a> Brabeeba might be interested in presenting.
			<li><a href="https://arxiv.org/abs/1803.09574">Long short-term memory and learning-to-learn in networks of spiking neurons</a>, Quanquan?</li>
			<li><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004347">Decreasing-Rate Pruning Optimizes the Construction of Efficient and Robust Distributed Networks</a>, Navlakha, Barth, Bar-Joseph.</li>
			<li>Other learning papers people are interested in?</li>
			</ul>
		</p>-->
	  </section>
	</div>
</div>
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./jquery-1.11.1.min.js"></script>
    <script src="./bootstrap.min.js"></script>

</div>
</body>
</html>